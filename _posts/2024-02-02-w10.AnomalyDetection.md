---
title: "Anomaly detection"
categories:
  - MachineLearning 
excerpt: "e.g., Fraud detection, unseen defects in manufacturing, monitoring machines in a data center"
author_profile: false
sidebar:
    title: Machine Learning
    nav: sidebar_ML
usemathjax: true
---

## Anomaly detection example

- $$x^{(i)}$$ = features of user $$i$$'s activities
- Model $$p(x)$$ from data
- Identify unusual users by checking which have $$p(x)<\epsilon$$

## Density estimation

- Training set: {$$\vec{x}^{(1)},\vec{x}^{(2)},...,\vec{x}^{(m)}$$}
- Each example $$\vec{x}^{(i)}$$ has $$n$$ features
- $$p(\vec{x})=p(x_{1};\mu_{1},\sigma_{1}^{2})*p(x_{2};\mu_{2},\sigma_{2}^{2})*\ldots*p(x_{n};\mu_{n},\sigma_{n}^{2})=$$ $$\prod^{n}_{j=1}p(x_{j};\mu_{j};\sigma_{j}^{2})$$

## Anomaly detection algorithm

1.  Choose $$n$$ features $$x_i$$ that you think might be indicative of anomalous examples
2.  Fit parameters $$\mu_{1},...,\mu_{n},\sigma_{1}^{2},...\sigma_{n}^{2}$$
    - $$\mu_{j}=\frac{1}{m}\sum^{m}_{i=1}x_{j}^{(i)}$$, $$\sigma_{j}^{2}=\frac{1}{m}\sum^{m}_{i=1}(x_{j}^{(i)}-\mu_{j})^{2}$$
    - $$\vec{\mu}=\frac{1}{m}\sum^{m}_{i=1}\vec{x}^{(i)}$$
3.  Given new example $$x$$, compute $$p(x)$$
    - $$p(x)=\prod^{n}_{j=1}p(x_{j};\mu_{j},\sigma_{j}^{2})=$$ $$\prod^{n}_{j=1}\frac{1}{\sqrt{2\pi}\sigma_{j}}exp(-\frac{(x_{j}-\mu_{j})^2}{2\sigma_{j}^{2}})$$
4.  Anomaly if $$p(x)<\epsilon$$

## Algorithm evaluation

- Fit model $$p(x)$$ on training set $$x^{(1)},x^{(2)},...,x^{(m)}$$
- On a cross validation/test example $$x$$, predict
  - $$y=1$$, if $$p(x)<\epsilon$$ (anomaly)
  - $$y=0$$, if $$p(x)>\epsilon$$ (normal)
- Possible evaluation metrics:
  - True positive, false positive, false negative, true negative
  - Precision/Recall
  - $$F_{1}$$ score
- Use cross validation set to choose parameter $$\epsilon$$

## Anomaly detection vs.Â Supervised learning

- Anomaly detection
  - very small number of positive examples ($$y=1$$). (0-20 is common)
  - large number of negative ($$y=0$$) examples
  - Many different "types" of anomalies. Hard for any algorithm to learn from positive examples what the anomalies look like; future anomalies may look nothing like any of the anomalies examples we've seen so far
  - Fraud detection, unseen defects in manufacturing, monitoring machines in a data center
- Supervised learning
  - large number of positive and negative examples
  - enough positive examples for algorithm to get a sense of what positive examples are like, future positive examples likely to be similar to ones in training set
  - email spam classification, known defects in manufacturing, weather prediction, disease classification

## Non-gaussian features

- Transforming values into logarithmic ones can shift a skewed distribution towards a bell-shaped distribution.
