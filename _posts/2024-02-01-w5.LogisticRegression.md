---
title: "Logistic Regression"
categories:
  - MachineLearning 
excerpt: "A post with custom sidebar content."
author_profile: false
sidebar:
    nav: sidebar_ML
usemathjax: true
---


## Sigmoid function

- logistic function
- outputs between 0 and 1
- $$g(z) = \frac{1}{1+e^{-z}}$$, $$0<g(z)<1$$

## Interpretation of logistic regression output

- $$f_{\vec w,b}(\vec x) = g(\vec{w} \cdot \vec{x}+b) = \frac{1}{1+e^{-(\vec{w} \cdot \vec{x} + b)}}$$
- "probability" that class is 1
  - $$f_{\vec w,b}(\vec x) = P(y=1|\vec{x};\vec{w},b)$$
  - probability that $$y$$ is 1, given input $$\vec{x}$$, parameters $$\vec{w}, b$$

## Decision boundary

- $$f_{\vec w,b}(\vec x) = g(\vec{w} \cdot \vec{x}+b) = \frac{1}{1+e^{-(\vec{w} \cdot \vec{x} + b)}}  = P(y=1|\vec{x};\vec{w},b)$$
- $$\hat{y}=1$$ when $$f_{\vec w,b}(\vec x) \geq 0.5$$
  - $$g(z) \geq 0.5$$
  - $$z\geq 0$$
  - $$\vec{w} \cdot \vec{x} + b \geq 0$$

## Cost function for logistic regression

- **Squared error cost** used for linear regression is not a good choice for logistic regression
  - In logistic regression, this is not convex, so easy to be fall into a local minima
- Logistic loss function
  - $$L(f_{\vec{w},b}(x^{(i)},y^{(i)}))$$
    - $$-log(f_{\vec{w},b}(\vec{x}^{(i)})$$, if $$y^{(i)}=1$$
    - $$-log(1-f_{\vec{w},b}(\vec{x}^{(i)})$$, if $$y^{(i)}=0$$
  - The further prediction $$f_{\vec{w},b}(\vec{x}^{(i)})$$ is from target $$y^{(i)}$$, the higher loss
- Cost function for logistic regression
  - $$J(\vec{w},b)=\frac{1}{m}\sum^{m}_{i=1}L(f_{\vec{w},b}(\vec{x}^{(i)},y^{(i)}))$$

## Simplified cost function for logistic regression

- $$L(f_{\vec{w},b}(x^{(i)},y^{(i)}))$$
  - $$-log(f_{\vec{w},b}(\vec{x}^{(i)})$$, if $$y^{(i)}=1$$
  - $$-log(1-f_{\vec{w},b}(\vec{x}^{(i)})$$, if $$y^{(i)}=0$$
- since $$y^{(i)}$$ could be only 1 or 0, this is the same as below
  - $$L(f_{\vec{w},b}(x^{(i)},y^{(i)}))=-y^{(i)}log(f_{\vec{w},b}(\vec{x}^{(i)})-(1-y^{(i)})log(1-f_{\vec{w},b}(\vec{x}^{(i)})$$
- $$J(\vec{w},b)=\frac{1}{m}\sum^{m}_{i=1}L(f_{\vec{w},b}(\vec{x}^{(i)},y^{(i)}))$$
- $$J(\vec{w},b)=-\frac{1}{m}\sum^{m}_{i=1}[y^{(i)}log(f_{\vec{w},b}(\vec{x}^{(i)})-(1-y^{(i)})log(1-f_{\vec{w},b}(\vec{x}^{(i)})]$$
  - this function is convex, therefore it is possible to find a global minimum

## Gradient descent for logistic regression

- $$J(\vec{w},b)=-\frac{1}{m}\sum^{m}_{i=1}[y^{(i)}log(f_{\vec{w},b}(\vec{x}^{(i)})-(1-y^{(i)})log(1-f_{\vec{w},b}(\vec{x}^{(i)})]$$
- When $$y=log(f(x))$$ , $$y' = \frac{f^{'}(x)}{f(x)}$$
- repeat
  - $$w_{j}=w_{j}-\alpha\frac{\partial}{\partial w_{j}}J(\vec{w},b)$$
    - $$\frac{\partial}{\partial w_{j}}J(\vec{w},b)=\frac{1}{m}\sum^{m}_{i=1}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})x_{j}^{(i)}$$
  - $$b=b-\alpha\frac{\partial}{\partial b}J(\vec{w},b)$$
    - $$\frac{\partial}{\partial b}J(\vec{w},b)=\frac{1}{m}\sum^{m}_{i=1}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})$$
- $$f_{\vec w,b}(\vec x) = g(\vec{w} \cdot \vec{x}+b) = \frac{1}{1+e^{-(\vec{w} \cdot \vec{x} + b)}}$$

## The problem of overfitting

- Underfit: does not fit the training set well
  - high bias
- Good fit: fits training set pretty well.
  - generalization
- Overfit: fits the training set extremely well
  - high variance

## Addressing overfitting

- Collect more training data
- Select features to include/exclude
- Regularization: reduce the size of parameters $$w_{j}$$

## Cost function with regularization

- $$J(\vec{w},b)=\frac{1}{2m}\sum^{m}_{i=1}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2 + \frac {\lambda}{2m} \sum^{n}_{j=1}{w_{j}}^2$$
- $$\lambda$$: regularization parameter ($$\lambda > 0$$)

## Regularized linear regression

- repeat
  - $$w_{j}=w_{j}-\alpha\frac{\partial}{\partial w_{j}}J(\vec{w},b)$$
    - $$\frac{\partial}{\partial w_{j}}J(\vec{w},b)=\frac{1}{m}\sum^{m}_{i=1}[(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})x_{j}^{(i)}] + \frac {\lambda}{m} \vec{w_{j}}$$
  - $$b=b-\alpha\frac{\partial}{\partial b}J(\vec{w},b)$$
    - $$\frac{\partial}{\partial b}J(\vec{w},b)=\frac{1}{m}\sum^{m}_{i=1}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})$$

## Regularized logistic regression

- $$f_{\vec w,b}(\vec x) = g(\vec{w} \cdot \vec{x}+b) = \frac{1}{1+e^{-(\vec{w} \cdot \vec{x} + b)}}$$
- repeat
  - $$w_{j}=w_{j}-\alpha\frac{\partial}{\partial w_{j}}J(\vec{w},b)$$
    - $$\frac{\partial}{\partial w_{j}}J(\vec{w},b)=\frac{1}{m}\sum^{m}_{i=1}[(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})x_{j}^{(i)}] + \frac {\lambda}{m} \vec{w_{j}}$$
  - $$b=b-\alpha\frac{\partial}{\partial b}J(\vec{w},b)$$
    - $$\frac{\partial}{\partial b}J(\vec{w},b)=\frac{1}{m}\sum^{m}_{i=1}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})$$
