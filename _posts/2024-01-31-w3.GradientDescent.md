---
title: "Gradient Descent"
categories:
  - MachineLearning 
excerpt: "A post with custom sidebar content."
author_profile: false
sidebar:
    nav: sidebar_ML
usemathjax: true
---

## Gradient descent algorithm
- $$w = w-\alpha\frac{d}{dw}J(w,b)$$
- $$b = b-\alpha\frac{d}{db}J(w,b)$$
- $$\alpha$$: learning rate
- repeat until convergence
- simultaneously update $$w$ and $b$$
	- tmp_w = $$w-\alpha\frac{\partial}{\partial w}J(w,b)$$
	- tmp_b = $$b-\alpha\frac{\partial}{\partial b}J(w,b)$$
	- $$w$$ = tmp_w, $$b = tmp\textunderscore b$$

## Learning rate
- if $$\alpha$$ is too small, gradient descent may be slow
- if $$\alpha$$ is too large, gradient descent may
	- overshoot, never reach minimum
	- fail to converge, diverge
- can reach local minimum with fixed learning rate
	- near a local minimum, derivatives becomes smaller, update steps become smaller
	- can reach minimum without decreasing learning rate $$\alpha$$

## partial derivative
- $$f(g(x))'$$ = $$f'(g(x))g'(x)$$ 
- $$\frac{\partial}{\partial w}J(w,b)$$ = $$\frac{\partial}{\partial w}\frac{1}{2m}\sum^{m}_{i=1}(f_{w,b}(x^{(i)})-y^{(i)})^{2}$$ = $$\frac{\partial}{\partial w}\frac{1}{2m}\sum^{m}_{i=1}(wx^{(i)}+b-y^{(i)})^{2}$$ = $$\frac{1}{2m}\sum^{m}_{i=1}(wx^{(i)}+b-y^{(i)})2x^{(i)}$$ =$$\frac{1}{m}\sum^{m}_{i=1}(f_{w,b}(x^{(i)})-y^{(i)})x^{(i)}$$ 
- $$\frac{\partial}{\partial b}J(w,b)$$ = $$\frac{\partial}{\partial w}\frac{1}{2m}\sum^{m}_{i=1}(f_{w,b}(x^{(i)})-y^{(i)})^{2}$$ = $$\frac{\partial}{\partial b}\frac{1}{2m}\sum^{m}_{i=1}(wx^{(i)}+b-y^{(i)})^{2}$$ = $$\frac{1}{2m}\sum^{m}_{i=1}(wx^{(i)}+b-y^{(i)})2$ =$\frac{1}{m}\sum^{m}_{i=1}(f_{w,b}(x^{(i)})-y^{(i)})$$

## Batch gradient descent
- **"Batch"**: each step of gradient descent uses all the training examples
