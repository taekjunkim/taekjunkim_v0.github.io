---
title: "Decision Tree"
categories:
  - MachineLearning 
excerpt: "Decision tree, tree ensemble"
author_profile: false
sidebar:
    title: Machine Learning
    nav: sidebar_ML
usemathjax: true
---


## Decision learning training

1.  How to choose what feature to split on at each node?
    - Maximum purity (or minimize impurity)
2.  When do you stop splitting?
    - When a node is 100% one class
    - When splitting a node will result in the tree exceeding a maximum depth
    - When improvements in purity score are below a threshold
    - When number of examples in a node is below a threshold

## Measuring purity

- Entropy as a measure of purity
  - $$p_1$$ = fraction of examples that are cats
  - $$p_{0} = 1 - p_{1}$$
  - $$H(p_{1}) = -p_{1}\log_{2}(p_{1}) - p_{0}\log_{2}(p_{0})$$
  - $$H(p_{1}) = -p_{1}\log_{2}(p_{1}) - (1 - p_{1})\log_{2}(1 - p_{1})$$

## Choosing a split: information gain

- Information gain
  - $$H(p_{1}^{root}) - (w^{left}H(p_{1}^{left})+w^{right}H(p_{1}^{right}))$$

## Decision tree learning

- Start with all examples at the root node
- Calculate information gain for all possible features, and pick the one with the highest information gain
- Split dataset according to selected feature, and create left and right branches of the tree
- Keep repeating splitting process until stopping criteria is met: **recursive splitting**
  - when a node is 100% one class
  - when splitting a node will result in the tree exceeding a maximum depth
  - information gain from additional splits is less than threshold
  - when number of examples in a node is below a threshold

## One hot encoding

- If a categorical feature can take on $$k$$ values, create $$k$$ binary features (0 or 1 valued)
  \## Decision tree for regression
- Choose a split which brings the largest variance decrease
  - $$Var^{root} - (w^{left}var^{left}+w^{right}var^{right})$$

## Using multiple decision trees

- Trees are highly sensitive to small changes of the data
- Tree ensembles
  - sampling with replacement

## Random forest algorithm

- Generating a tree sample
  - Given training set of size $$m$$
  - For $$b$$ = 1 to $$B$$
    - use sampling with replacement to create a new training set of size $$m$$
    - train a decision tree on the new dataset
  - Bagged decision tree
- Randomizing the feature choice
  - At each node, when choosing a feature to use to split, if $$n$$ features are available, pick a random subset of $$k < n$$ features and allow the algorithm to only choose from that subset of features
    - $$k = \sqrt{n}$$

## Boosted trees intuition

- Given training set of size $$m$$
- For $$b$$ = 1 to $$B$$:
  - use sampling with replacement to create a new training set of size $$m$$
    - But instead of picking from all examples with equal ($$1/m$$) probability, make it more likely to pick misclassified examples from previous training trees
  - train a decision tree on the new dataset

## XGBoost (eXtreme Gradient Boosting)

- Open source implementation of boosted trees
- Fast efficient implementation
- Good choice of default splitting criteria and criteria for when to stop splitting
- Built in regularization to prevent overfitting
- Highly competitive algorithm for machine learning competitions

## Using XGBoost

- Classification

``` python
from xgboost import XGBClassifier

model = XGBClassifier()

model.fit(X_train, Y_train)
y_pred = model.predict(X_test)
```

- Regression

``` python
from xgboost import XGBRegressor

model = XGBRegressor()

model.fit(X_train, Y_train)
y_pred = model.predict(X_test)
```

## Decision trees vs.Â Neural networks

- Decision trees and tree ensembles
  - works well on tabular (structured) data
  - Not recommended for unstructured data (images, audio, text)
  - Fast
  - Small decision trees may be human interpretable
- Neural networks
  - works well on all types of data, including tabular (structured) and unstructured data
  - May be slower than a decision tree
  - Works with transfer learning
  - When building a system of multiple models working together, it might be easier to string together multiple neural networks
