---
title: "Entropy, Mutual information"
categories:
  - Neuroscience 
excerpt: "Entropy, Mutual information"
author_profile: false
sidebar:
    title: Computational Neuroscience
    nav: sidebar_CN
usemathjax: true
---


## Decoding

- How well can we learn what the stimulus is by looking at the neural responses?
- Decoding from many neurons: population codes
  - population vector
  - Bayesian inference
  - maximum likelihood
  - maximum a posteriori

## Bayesian inference

- Bayes' law: $$p[s|r]=\frac{p[r|s] \ p[s]}{p[r]} = posterior = \frac{likelihood \ \cdot \ prior}{marginal}$$

## Information

- P(1) = p
- P(0) = 1-p
- Information (1) = $$-log_{2}(p)$$
- Information (0) = $$-log_{2}(1-p)$$

## Entropy

- average information
- $$H = -\sum p_{i} \ log_{2}\ {p_{i}}$$
- Entropy in binary systems is closely related to the amount of information conveyed by each outcome. When the entropy is high, each outcome provides a significant amount of information because the outcome is uncertain. Conversely, when the entropy is low, the amount of information provided by each outcome decreases because the outcome is more predictable.

## Mutual information

- $$I(R,S) = -\sum_{r} p(r) \ log_{2}p(r) - \sum_{s} p(s)[-\sum_{r}p(r|s)log_{2}p(r|s)]=H(R)-\sum_{s}p(s)H[R|s]$$
  - total response entropy - the mean noise entropy
- $$I(R,S)=H[R]-\sum_{s}P(s)H[R|s]=H[S]-\sum_{r}P(r)H[S|r]$$
  - take one stimulus $$s$$ and repeat many times to obtain $$P[R|s]$$
  - compute variability due to noise: noise entropy $$H[R|s]$$
  - repeat for all $$s$$ and average: $$\sum_{s}P(s)H[R|s]$$
  - compute $$P(R)=\sum_{s}P(s)P(r|s)$$ and total entropy $$H(R)$$
- $$I(X;Y)=H(X)−H(X∣Y)$$
- mutual information measures how much the uncertainty about one variable decreases when we know the value of the other variable. If $$I(X;Y)$$ is high, it means that knowing the value of $$Y$$ provides a lot of information about $$X$$, and vice versa. If $$I(X;Y)$$ is low or zero, it means that knowing the value of one variable doesn't tell us much about the other variable, indicating that they are independent or unrelated.

## Efficient coding

- In order to encode stimuli effectively, an encoder should match its outputs to the statistical distribution of the inputs

## Coding principles

- Coding efficiency
- Adaptation to stimulus statistics
- Sparseness: reduce redundancy
