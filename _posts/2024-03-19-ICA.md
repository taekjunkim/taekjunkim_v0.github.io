---
title: "Independent component analysis"
classes: wide 
categories:
  - MachineLearning
---


**independent component analysis** (**ICA**) is a computational method for separating a multi-variate signal into additive subcomponents. This is done by assuming that at most one subcomponent is Gaussian and that the subcomponents are statistically independent from each other.

Independent component analysis attempts to decompose a multivariate signal into independent non-Gaussian signals. The success of ICA separation of mixed signals relies on two assumptions and three effects of mixing source signals.
- Two assumptions:
1. The source signals are **independent** of each other.
2. The values in each source signal have **non-Gaussian** distributions.
- Three effects of mixing source signals:
1. Independence: As per assumption 1, the source signals are independent; however, their signal mixtures are not. This is because the signal mixtures share the same source signals.
2. Normality: According to the Central Limit Theorem, the distribution of a sum of independent random variables with finite variance tends towards a Gaussian distribution. Loosely speaking, a sum of two independent random variables usually has a distribution that is closer to Gaussian than any of the two original variables. Here we consider the value of each signal as the random variable.
3. Complexity: The temporal complexity of any signal mixture is greater than that of its simplest constituent source signal.

We may choose one of many ways to define a proxy for independence, and this choice governs the form of the ICA algorithm. The two broadest definitions of independence for ICA are
1. Minimization of mutual information
- Algorithms uses measures like **Kullback-Leibler Divergence** and maximum entropy.
- **KL divergence**
- $$D_{KL}(P||Q)=\sum P(x) log(\frac{P(x)}{Q(x)})$$
- How one probability distribution $$P$$ is different from a second, reference probability distribution $$Q$$
- A simple interpretation of the KL divergence of P from Q is the expected excess surprise from using Q as a model when the actual distribution is P

        - **entropy**
            - surprise: high when probability is low
                = $$log\frac{1}{p(x)} = -log(p(x))$$
            - entropy: expected surprise for a series of events
                = $$-\sum p(x)log(p(x))$$

1.  Maximization of non-Gaussianity
    - motivated by the central limit theorem, uses kurtosis and negentropy.

Typical algorithms for ICA use centering (subtract the mean to create a zero mean signal), whitening (usually with the eigenvalue decomposition), and dimensionality reduction as preprocessing steps in order to simplify and reduce the complexity of the problem for the actual iterative algorithm. Whitening and dimension reduction can be achieved with principal component analysis or singular value decomposition. Whitening ensures that all dimensions are treated equally *a priori* before the algorithm is run. In general, ICA cannot identify the actual number of source signals, a uniquely correct ordering of the source signals, nor the proper scaling (including sign) of the source signals.
- **Whitening**:  linear transformation that transforms a vector of random variables with a known covariance matrix into a set of new variables whose covariance is the identity matrix, meaning that they are uncorrelated and each have variance 1. The transformation is called "whitening" because it changes the input vector into a white noise vector.
- Suppose $$X$$ is a random (column) vector with non-singular covariance matrix $$\sum$$ and mean 0. Then transformation $$Y = WX$$ with a whitening matrix $$W$$ satisfying the condition $$W^{T}W=\sum^{-1}$$ yields the whitened random vector $$Y$$ with unit diagonal covariance.
