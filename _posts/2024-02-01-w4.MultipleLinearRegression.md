---
title: "Multiple Linear Regression"
categories:
  - MachineLearning 
excerpt: "A post with custom sidebar content."
author_profile: false
sidebar:
    nav: sidebar_ML
usemathjax: true
---


## Model

- $$f_{w,b}(x) = w_{1}x_{1} + w_{2}x_{2} + w_{1}x_{1} + ... + w_{n}x_{n} + b$$
  - $$\vec{w} = [w_{1}\ w_{2}\ w_{3}\ ...\ w_{n}]$$
  - $$\vec{x} = [x_{1}\ x_{2}\ x_{3}\ ...\ x_{n}]$$
- $$f_{\vec{w},b}(\vec{x}) = w_{1}x_{1} + w_{2}x_{2} + w_{1}x_{1} + ... + w_{n}x_{n} + b = \vec{w} \cdot \vec{x} + b$$

## Vectorization

- Without vectorization
  - $$f_{\vec{w},b}(\vec{x}) = w_{1}x_{1} + w_{2}x_{2} + w_{1}x_{1} + ... + w_{n}x_{n} + b$$
```python
f = 0
for j in range(0,n):
    f = f + w[j] * x[j]
f = f + b
```

- Vectorization
  - $$f_{\vec{w},b}(\vec{x}) = \vec{w} \cdot \vec{x} + b$$
```python
f = np.dot(w,x) + b
```

## Gradient descent for multiple linear regression

- $$f_{\vec{w},b}(\vec{x}) = \vec{w} \cdot \vec{x} + b$$
- Gradient descent
  - repeat:
    - $$w_{1} = w_{1} - \alpha \frac{\partial}{\partial w_{1}}J(\vec w,b)$$
    - $$w_{2} = w_{2} - \alpha \frac{\partial}{\partial w_{2}}J(\vec w,b)$$
    - ...
    - $$w_{n} = w_{n} - \alpha \frac{\partial}{\partial w_{n}}J(\vec w,b)$$
    - $$b = b - \alpha \frac{\partial}{\partial b}J(\vec w,b)$$

## An alternative to gradient descent

- Normal equation
  - Only for linear regression
  - Solve for $$w, b$$ without iterations
  - Disadvantages
    - Doesn't generalize to other learning algorithms
    - Slow when number of features is large (\>10,000)
  - This method may be used in machine learning libraries that implement linear regression
  - Gradient descent is the recommended method for finding parameters $$w,b$$

## Gradient descent in practice

- Feature scaling
  - aim for about $$-1 \leq x_{j} \leq 1$$ for each feature $$x_{j}$$
  - small deviations are acceptable, but need to avoid too large or too small feature

## Checking gradient descent for convergence

- With a small enough $$\alpha$$, $$J(\vec {w}, b)$$ should decrease on every iteration
- If $$\alpha$$ is too small, gradient descent takes a lot more iterations to converge
- Values of $$\alpha$$ to try:
  - 0.001 --\> 0.003 --\> 0.01 --\> 0.03 --\> ...

## Feature engineering

- Using intuition to design new features, by transforming or combining original features
- **Polynomial regression**: feature scaling needs to follow
