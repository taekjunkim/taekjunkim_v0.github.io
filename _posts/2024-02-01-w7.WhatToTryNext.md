---
title: "Decide what to try next"
categories:
  - MachineLearning 
excerpt: "Debugging a learning algorithm"
author_profile: false
sidebar:
    title: Machine Learning
    nav: sidebar_ML
usemathjax: true
---


## Debugging a learning algorithm

- You've implemented regularized linear regression on housing prices
$$
J(\vec{w},b)=\frac{1}{2m}\sum^{m}_{i=1}(f_{\vec{w},b}(\vec{x}^{(i)}-y^{(i)}))^2 + \frac{\lambda}{2m}\sum^{n}_{j=1}\vec{w}_{j}^{2}
$$
- But it makes unacceptably large errors in predictions. What do you try next?
  - get more training examples: fixes high variance
  - try smaller set of features: fixes high variance
  - try getting additional features: fixes high bias
  - try adding polynomial features ($$x_{1}^2, x_{2}^2, x_1, x_2, etc$$): fixes high bias
  - try decreasing $$\lambda$$: fixes high bias
  - try increasing $$\lambda$$: fixes high variance

## Machine learning diagnostic

- A test that you run to gain insight into what is/isn't working with a learning algorithm, to gain guidance into improving its performance

## Model selection and training/cross validation/test sets

- Once parameters $$\vec{w},b$$ are fit to the training set, the training error $$J_{train}(\vec{w},b)$$ is likely lower than the actual generalization error
- $$J_{test}(\vec{w},b)$$ is better estimate of how well the model will generalize to new data
- Divide dataset into three groups: training / cross-validation / test sets
  - pick a model based on cross-validation error
  - estimate generalization error using the test set

## Diagnosing bias and variance

- High bias: underfit
  - cost function: $$J_{train}$$ is high, $$J_{cv}$$ is high
- High variance: overfit
  - cost function: $$J_{train}$$ is low, $$J_{cv}$$ is high

## Regularization and bias/variance

$$
J(\vec{w},b) = \frac{1}{2m}\sum^{m}_{i=1}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2 + \frac{\lambda}{2m}\sum^{n}_{j=1}w_{j}^2 
$$
- too large $$\lambda$$ may result in high bias (underfit)
- too small $$\lambda$$ may result in high variance (overfit)
- Choosing the regularization parameter $$\lambda$$
- try different $$\lambda$$ values, and choose one based on $$J_{cv}(\vec{w},b)$$

## Establishing a baseline level of performance

- What is the level of error you can reasonably hope to get to?
  - human level performance
  - competing algorithms performance
  - guess based on experience

## Learning curves

- As training set size gets bigger
  - $$J_{cv}(\vec{w},b)$$ is getting smaller
  - $$J_{train}(\vec{w},b)$$ is getting larger
- if a learning algorithm suffers from high bias, getting more training data will not (by itself) help much
- if a learning algorithm suffers from high variance, getting more training data is likely to help

## bias/variance and neural networks

- Large neural networks are low bias machines
- Does it do well on the training set?
  - No: try bigger network
  - Yes. then, does it do well on the cross-validation set?
    - No: get more data
    - Yes: done
- A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately
  - Unregularized MNIST model

``` python
layer_1 = Dense(units=25, activation='relu')
layer_2 = Dense(units=15, activation='relu')
layer_3 = Dense(units=1, activation='sigmoid')
model = Sequential([layer_1,layer_2,layer_3])
```

  - Regularized MNIST model

``` python
layer_1 = Dense(units=25, activation='relu', kernel_regularizer=L2(0.01))
layer_2 = Dense(units=15, activation='relu', kernel_regularizer=L2(0.01))
layer_3 = Dense(units=1, activation='sigmoid', kernel_regularizer=L2(0.01))
model = Sequential([layer_1,layer_2,layer_3])
```

## Iterative loop of ML development

1.  Choose architecture
    - model, data, etc
2.  Train model
3.  Diagnostics (bias, variance, and error analysis)
4.  Repeat 1-3

## Adding data

- Add more data of everything
- Add more data of the types where error analysis has indicated it might help
- Beyond getting brand new training examples (x,y), another technique: **data augmentation**
- Data augmentation
  - modifying an existing training example to create a new training example
  - introducing distortion
    - distortion introduced should be representation of the type of noise/distortions in the test set
    - usually does not help to add purely random/meaningless noise to your data
- Data synthesis
  - using artificial data inputs to create a new training example
  - model-centric approach vs.Â data-centric approach

## Transfer learning: using data from a different task

- Option 1: only train output layers parameters
- Option 2: train all parameters

1.  Download neural network parameters pretrained on a large dataset with same input type (e.g., images, audio, text) as your application (or train your own)
2.  Further train (fine tune) the network on your own data

## Full cycle of a machine learning project

1.  Scope project: define project
2.  Collect data: define and collect data
3.  Train model: training, error analysis & iterative improvement
    - return to 2 when needed
4.  Deploy in production: deploy, monitor and maintain system
    - return to 2 or 3 when needed

## Deployment

- Software engineering may be needed for
  - ensure reliable and efficient predictions
  - scaling
  - logging
  - system monitoring
  - model updates

## Guidelines for fairness, bias, and ethics issue

- get a diverse team to brainstorm things that might go wrong, with emphasis on possible harm to vulnerable groups
- carry out literature search on standards/guidelines for your industry
- audit systems against possible harm prior to deployment
- develop mitigation plan (if applicable), and after deployment, monitor for possible harm

## Error metrics for skewed datasets

- **Precision**: of all patients where we predicted $$y=1$$, what fraction actually have the rare disease?
$$
\frac{True \ positives}{Predicted \ positives}=\frac{True \ positives}{TP + FP}
$$
- **Recall**: of all patients that actually have the rare disease, what fraction did we correctly detect as having it?
$$
\frac{True \ positives}{Actual \ positives}=\frac{True \ positives}{TP + FN}
$$

## Trading off precision and recall

- $$F_1$$ score: harmonic mean of precision(P) and recall(R)
$$
F_1\ score = \frac {1}{\frac{1}{2}(\frac{1}{P}+\frac{1}{R})} = \frac{2PR}{P+R}
$$
