---
title: "TensorFlow Implementation"
categories:
  - MachineLearning 
excerpt: "A post with custom sidebar content."
author_profile: false
sidebar:
    nav: sidebar_ML
usemathjax: true
---


## Given set of (x,y) examples, how to build and train a model

``` python
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

model = Sequential([
    Dense(units=25, activation='sigmoid'),
    Dense(units=15, activation='sigmoid'),                      
    Dense(units=1, activation='sigmoid'),   
])

from tensorflow.keras.losses import BinaryCrossentropy
model.compile(loss=BinaryCrossentropy())

model.fit(X,Y,epochs=100)
```

- epochs: number of steps in gradient descent

## Model training steps

1.  Specify how to compute output given input $$x$$ and parameters $$w,b$$ : define model
    - $$f_{\vec{w},b}(\vec{x}) =\ ?$$
    - Logistic regression

    ``` python
          z = np.dot(w,x) + b
          f_x = 1/(1+np.exp(-z))
    ```
2.  Specify loss and cost
    - $$L(f_{\vec{w},b}(\vec{x}),y)$$
    - $$J(\vec{w},b)=\frac{1}{m}\sum^{m}_{i=1}L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})$$
    - Logistic loss
      `python      loss =      -y * np.log(f_x)               -(1-y) * np.log(1 - f_x)`
3.  Train on data to minimize $$J(\vec{w},b)$$
    `python     w = w - alpha * dj_dw     b = b - alpha * dj_db`

## In TensorFlow

``` python
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

model = Sequential([
    Dense(units=25, activation='sigmoid'),
    Dense(units=15, activation='sigmoid'),                      
    Dense(units=1, activation='sigmoid'),   
])

from tensorflow.keras.losses import BinaryCrossentropy
model.compile(loss=BinaryCrossentropy())  # for logistic regression

#from tensorflow.keras.losses import MeanSquaredError
#model.compile(loss=MeanSquaredError())  # for predicting numbers and not categories

# compute derivatives for gradient descent using "backpropagation"
model.fit(X,y,epoch=100)
```

## Alternatives to the sigmoid activation

- ReLU: rectified linear unit
  - $$g(z) = max(0,z)$$

## Choosing activation functions

- Binary classification
  - activation = 'sigmoid'
- regression
  - for y with both positive and negative: activation = 'linear'
    - don't use 'linear' activation function in hidden layer
  - for y with positive only, and hidden layer: activation = 'relu'

## Multi-class classification

- logistic regression (2 possible output values)
  - $$z=\vec{w} \cdot \vec{x}+b$$
  - $$a_{1}=g(z)=\frac{1}{1+e^{-z}}=P(y=1|\vec{x})$$
  - $$a_{0}=1-a_{1}=P(y=0|\vec{x})$$
- Softmax regression (e.g., 4 possible outputs)
  - $$z_{1}=\vec{w}_{1} \cdot \vec{x}+b_{1}$$
  - $$z_{2}=\vec{w}_{2} \cdot \vec{x}+b_{2}$$
  - $$z_{3}=\vec{w}_{3} \cdot \vec{x}+b_{3}$$
  - $$z_{4}=\vec{w}_{4} \cdot \vec{x}+b_{4}$$
  - $$a_{1}=\frac{e^{z_{1}}}{e^{z_{1}}+e^{z_{2}}+e^{z_{3}}+e^{z_{4}}} = P(y=1|\vec{x})$$
  - $$a_{2}=\frac{e^{z_{2}}}{e^{z_{1}}+e^{z_{2}}+e^{z_{3}}+e^{z_{4}}} = P(y=2|\vec{x})$$
  - $$a_{3}=\frac{e^{z_{3}}}{e^{z_{1}}+e^{z_{2}}+e^{z_{3}}+e^{z_{4}}} = P(y=3|\vec{x})$$
  - $$a_{4}=\frac{e^{z_{4}}}{e^{z_{1}}+e^{z_{2}}+e^{z_{3}}+e^{z_{4}}} = P(y=4|\vec{x})$$
- Softmax regression (N possible outputs)
  - $$z_{j} = \vec{w}_{j} \cdot \vec{x} + b_{j},\ j=1,2,...,N$$
  - $$a_{j}=\frac{e_{j}}{\sum^{n}_{k=1}e^{z_{k}}}$$
  - note: $$a_{1}+a_{2}+ \ldots +a_{n}=1$$
- Loss function of Softmax regression
  - $$loss(a_{1},a_{2},\ldots,a_{N},y)$$
    = $$-\log a_{1}$$ if $$y=1$$
    $$-\log a_{2}$$ if $$y=2$$
    $$\ldots$$
    $$-\log a_{N}$$ if $$y=N$$

## Neural network with SoftMax output

- MNIST with softmax

``` python
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

model = Sequential([
    Dense(units=25,activation='relu'),
    Dense(units=15,activation='relu'),                      
    Dense(units=10,activation='softmax'),                           
])

from tensorflow.keras.losses import SparseCategoricalCrossentropy
model.compile(loss=SparseCategoricalCrossentropy())
model.fit(X,Y,epoch=100)
```

## Improved implementation of softmax

- Numerical roundoff errors
  - more numerically accurate implementation of logistic loss
- Logistic regression
  - replace activation function in the top output layer with 'linear'
  - use **from_logits=True** in the loss function
  - **f_x** is defined outside of the model

  ``` python
  model = Sequential([
    Dense(units=25,activation='relu'),
    Dense(units=15,activation='relu'),
    #Dense(units=1,activation='sigmoid'),
    Dense(units=1,activation='linear'),
  ])
  #model.compile(loss=BinaryCrossEntropy())
  model.compile(loss=BinaryCrossEntropy(from_logits=True))
  model.fit(X,Y,epoch=100)
  logit = model(X)
  f_x = tf.nn.sigmoid(logit)
  ```
- SoftMax

``` python
model = Sequential([
    Dense(units=25,activation='relu'),
    Dense(units=15,activation='relu'),
    #Dense(units=10,activation='softmax'),
    Dense(units=10,activation='linear'),
])
#model.compile(loss=SparseCategoricalCrossEntropy())
model.compile(loss=SparseCategoricalCrossEntropy(from_logits=True))

model.fit(X,Y,epochs=100)
logits = model(X)
f_x = tf.nn.softmax(logits)
```

## Advanced optimization

- Gradient descent
  - $$w_{j}=w_{j}-\alpha \frac{\partial}{\partial w_{j}}J(\vec{w},b)$$
- Adam (Adaptive Momentum ) Algorithm intuition, not just one $$\alpha$$
  - $$w_{1}=w_{1}-\alpha_{1} \frac{\partial}{\partial w_{1}}J(\vec{w},b)$$
  - ...
  - $$w_{10}=w_{10}-\alpha_{10} \frac{\partial}{\partial w_{10}}J(\vec{w},b)$$
  - $$b=b-\alpha_{11} \frac{\partial}{\partial b}J(\vec{w},b)$$
  - if $$w_{j}$$ (or $$b$$) keeps moving in same direction, increase $$\alpha_{j}$$
  - if $$w_{j}$$ (or $$b$$) keeps oscillating, decrease $$\alpha_{j}$$
    \`\`\`python
    model = Sequential(\[
    Dense(units=25,activation='relu'),
    Dense(units=15,activation='relu'),
    \#Dense(units=10,activation='softmax'),
    Dense(units=10,activation='linear'),\])
    \#model.compile(loss=SparseCategoricalCrossEntropy())
    model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss=SparseCategoricalCrossEntropy(from_logits=True)
    )

model.fit(X,Y,epochs=100)
logits = model(X)
f_x = tf.nn.softmax(logits)
\`\`\`

## Additional layer types

- Dense layer: Each neuron output is a function of all the activation outputs of the previous layer
- Convolutional layer: each neuron only looks at part of the previous layer's outputs
  - faster computation
  - need less training data (less prune to overfitting)

## Chain rule (calculus)

$$
\begin{flalign}
& \frac{\partial J}{\partial a} = \frac{\partial d}{\partial a} \cdot \frac{\partial J}{\partial d} &
\end{flalign}
$$

## Backpropagation is an efficient way to compute derivatives

- $$J = \frac{1}{2}((wx+b)-y)^2$$ , where $$w=2, b=8, x=-2, y=2$$
- $$w$$ --\> $$c=wx$$ --\> $$a = c+b$$ --\> $$d = a - y$$ --\> $$J = \frac{1}{2}d^{2}$$
- $$\frac {\partial J}{\partial d} = 2$$
- $$\frac {\partial J}{\partial a} = \frac {\partial d}{\partial a} \cdot \frac {\partial J}{\partial d} =2$$
- $$\frac {\partial J}{\partial b} = \frac {\partial a}{\partial b} \cdot \frac {\partial J}{\partial a} =2$$
- $$\frac {\partial J}{\partial c} = \frac {\partial a}{\partial c} \cdot \frac {\partial J}{\partial a} =2$$
- $$\frac {\partial J}{\partial w} = \frac {\partial c}{\partial w} \cdot \frac {\partial J}{\partial c} =-4$$
- If $$N$$ nodes and $$P$$ parameters, compute derivatives in roughly $$N+P$$ steps rather than $$N$$ x $$P$$ steps
