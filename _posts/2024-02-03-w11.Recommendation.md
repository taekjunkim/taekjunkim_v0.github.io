---
title: "Recommendation system"
categories:
  - MachineLearning 
excerpt: "Collaborative filtering vs. Content-based filtering"
author_profile: false
sidebar:
    title: Machine Learning
    nav: sidebar_ML
usemathjax: true
---


## Cost function for parameters of users

- Notation
  - $$r(i,j)=1$$ if user $$j$$ has rated movie $$i$$ (0 otherwise)
  - $$y^(i,j)$$ = rating given by user $$j$$ on movie $$i$$ (if defined)
  - $$w^{(j)},b^{(j)}$$ = parameters of user $$j$$
  - $$x^{(i)}$$ = feature vector of movie $$i$$
  - For user $$j$$ and movie $$i$$, predict rating: $$w^{(j)} \cdot x^{(i)} + b^{j}$$
  - $$m^{(j)}$$ = number of movies rated by user $$j$$
- To learn parameters $$w^{(j)},b^{(j)}$$ for user $$j$$
  - $$\underset{w^{(j)},b^{(j)}}{min}J(w^{(j)},b^{(j)})= \frac{1}{2m^{(j)}}\sum_{i:r(i,j)=1}(w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i,j)})^{2} + \frac{\lambda}{2m^{(j)}}\sum^{n}_{k=1}(w_{k}^{(j)})^{2}$$
  - $$n$$: number of features
- To learn parameters $$w^{(1)},b^{(1)},w^{(2)},b^{(2)},...,w^{(n_u)},b^{(n_u)}$$ for all users
  - $$\underset{w^{(1)},b^{(1)},...,w^{(n_u)},b^{(n_u)}}{min}J(w^{(1)},...w^{(n_u)},b^{(1)},...,b^{(n_u)})= \frac{1}{2}\sum^{n_u}_{j=1}\sum_{i:r(i,j)=1}(w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i,j)})^{2} + \frac{\lambda}{2}\sum^{n_u}_{j=1}\sum^{n}_{k=1}(w_{k}^{(j)})^{2}$$

## Cost function for collaborative filtering

- Given $$w^{(1)},b^{(1)},...,w^{(n_u)},b^{(n_u)}$$
- To learn $$x^{(i)}$$
  - $$J(x^{(i)})=\frac{1}{2}\sum_{j:r(i,j)=1}(w^{j} \cdot x^{(i)}+b^{j} - y^{(i,j)})^{2} + \frac{\lambda}{2}\sum^{n}_{k=1}(x_{k}^{(i)})^2$$
- To learn $$x^{(1)},x^{(2)},...,x^{(n_m)}$$
  - $$\underset{x^{1},...,x^{n_m}}{min}J(x^{(1)},x^{(2)},...,x^{(n_m)})=\frac{1}{2}\sum^{n_m}_{i=1}\sum_{j:r(i,j)=1}(w^{j} \cdot x^{(i)}+b^{j} - y^{(i,j)})^{2} + \frac{\lambda}{2}\sum^{n_m}_{i=1}\sum^{n}_{k=1}(x_{k}^{(i)})^2$$
    \## Collaborative filtering, put together
- To learn parameters $$w^{(1)},b^{(1)},w^{(2)},b^{(2)},...,w^{(n_u)},b^{(n_u)}$$ for all users
  - $$\underset{w^{(j)},b^{(j)}}{min}J(w^{(1)},...w^{(n_u)},b^{(1)},...,b^{(n_u)})= \frac{1}{2}\sum^{n_u}_{j=1}\sum_{i:r(i,j)=1}(w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i,j)})^{2} + \frac{\lambda}{2}\sum^{n_u}_{j=1}\sum^{n}_{k=1}(w_{k}^{(j)})^{2}$$
- To learn $$x^{(1)},x^{(2)},...,x^{(n_m)}$$
  - $$\underset{x^{1},...,x^{n_m}}{min}J(x^{(1)},x^{(2)},...,x^{(n_m)})=\frac{1}{2}\sum^{n_m}_{i=1}\sum_{j:r(i,j)=1}(w^{j} \cdot x^{(i)}+b^{j} - y^{(i,j)})^{2} + \frac{\lambda}{2}\sum^{n_m}_{i=1}\sum^{n}_{k=1}(x_{k}^{(i)})^2$$
- Put them together
  - $$\underset{w^{(1)},...,w^{(n_u)},b^{(1)},...,b^{(n_u)},x^{(1)},...,x^{(n_m)}}{min}J(w,b,x)=\frac{1}{2}\sum^{n_m}_{i=1}\sum_{(i,j):r(i,j)=1}(w^{j} \cdot x^{(i)}+b^{j} - y^{(i,j)})^{2} + \frac{\lambda}{2}\sum^{n_u}_{j=1}\sum^{n}_{k=1}(w_{k}^{(j)})^2 + \frac{\lambda}{2}\sum^{n_m}_{i=1}\sum^{n}_{k=1}(x_{k}^{(i)})^2$$.
- Gradient descent
  - $$w_{i}^{(j)}=w_{i}^{(j)}-\alpha\frac{\partial}{\partial{w_{i}^{(j)}}}J(w,b,x)$$
  - $$b_{i}^{(j)}=b_{i}^{(j)}-\alpha\frac{\partial}{\partial{b_{i}^{(j)}}}J(w,b,x)$$
  - $$x_{k}^{(i)}=x_{k}^{(i)}-\alpha\frac{\partial}{\partial{x_{k}^{(i)}}}J(w,b,x)$$

## Cost function for binary application

- Previous cost function
  - $$\frac{1}{2}\sum^{n_m}_{i=1}\sum_{(i,j):r(i,j)=1}(w^{j} \cdot x^{(i)}+b^{j} - y^{(i,j)})^{2} + \frac{\lambda}{2}\sum^{n_u}_{j=1}\sum^{n}_{k=1}(w_{k}^{(j)})^2 + \frac{\lambda}{2}\sum^{n_m}_{i=1}\sum^{n}_{k=1}(x_{k}^{(i)})^2$$
- Loss for binary labels
  - $$y^{(i,j)}$$: $$f_{(w,b,x)}(x) = g(w^{(j)} \cdot x^{(i)} + b^{(j)})$$
  - Loss for single example: $$L(f_{(w,b,x)},y^{(i,j)})=-y^{(i,j)}log(f_{(w,b,x)(x)})-(1-y^{(i,j)})log(1-f_{(w,b,x)}(x))$$
  - cost for all examples: $$J(w,b,x) = \sum_{(i,j):r(i,j)=1}L(f_{(w,b,x)}y^{(i,j)})$$

## Mean normalization

1.  **Handling Biases:**
    - Users may have different rating scales, with some users tending to rate items more positively than others. Similarly, some items may receive consistently higher or lower ratings overall.
    - Mean normalization helps in addressing these biases by centering the ratings around zero. This means that the baseline rating for each user or item is considered, and the actual ratings are adjusted accordingly.
2.  **Comparability:**
    - Mean normalization makes ratings more comparable across different users and items. Without normalization, it might be challenging to compare ratings between users who use different parts of the rating scale.
    - By centering the ratings, the focus shifts to the relative preferences of users or items, making the recommendations more accurate and personalized.
3.  **Sparse Data Handling:**
    - In recommender systems, user-item matrices are often sparse, meaning that not all users have rated all items. Mean normalization can be particularly useful in such scenarios.
    - By centering the ratings, the system can better handle missing data and make predictions even when a user has not rated many items. This is especially important for collaborative filtering methods.
    - Without mean normalization, predictions may result in that a new user would rate all movies as zero, because regularization term in the cost function can lead $$w^{(j)},b^{(j)}$$ toward zero.
4.  **Improved Model Training:**
    - Normalizing the ratings can lead to better-behaved optimization landscapes during model training. It can help the learning algorithm converge faster and find more meaningful patterns in the data.
    - Normalization can mitigate issues related to varying scales and biases, making it easier for the model to learn the underlying patterns in user-item interactions.
5.  **Enhanced Recommendations:**
    - Mean normalization can lead to more accurate and personalized recommendations. By focusing on the relative preferences of users and items, the recommender system can better capture the nuances of user behavior.
    - Centering ratings around zero allows the system to distinguish between items that a user generally rates higher or lower than their average, leading to more tailored recommendations.

## Finding related items

- The feature $$x^{(i)}$$ of item $$i$$ are quite hard to interpret
- To find other items related to it, find item $$k$$ with $$x^{(k)}$$ similar to $$x^{(i)}$$
  - $$\sum^{n}_{l=1}(x_{l}^{(k)}-x_{l}^{(i)})^2=||x^{(k)}-x^{(i)}||^2$$

## Limitations of collaborative filtering

- Cold start problem. How to
  - rank new items that few users have rated?
  - show something reasonable to new users who have rated few items?
- Use side information about items or users
  - Item: Genre, movie stars, studio, ...
  - User: demographics (age, gender, location), expressed preferences, ...

## Collaborative filtering vs.Â Content-based filtering

- Collaborative filtering
  - Recommend items to you based on ratings of users who gave similar ratings as you
- Content-based filtering
  - Recommend items to you based on features of user and item to find good match
  - Example of user and item features
    - User features: age, gender, country, movie watched, average rating per genre
    - Movie features: year, genre, reviews, average rating

## Deep learning for Content-based filtering

- User network: from $$X_u$$ to $$V_u$$
  - $$v_u^{(j)}$$: vector of length N that describes user $$j$$ with features $$x_u^{(j)}$$
- Movie network: from $$X_m$$ to $$V_m$$
  - $$v_m^{(i)}$$: vector of length N that describes movie $$i$$ with features $$x_m^{(i)}$$
- To find movies similar to movie $$i$$
  - $$||V_m^{(k)}-V_m^{(i)}||^{2}$$ is small
- Prediction: $$g(V_u^{(j)} \cdot V_m^{(i)})$$ to predict the probability that $$y^{(i,j)}=1$$
- Cost function: $$J=\sum_{(i,j):r(i,j)=1}(V_u^{(j)} \cdot V_m^{(i)} - y^{(i,j)})^2$$ + NN regularization term

## TensorFlow implementation of content-based learning

``` python
user_NN = tf.keras.models.Sequential([
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(128, activation='relu'),                          
    tf.keras.layers.Dense(32),    
])
item_NN = tf.keras.models.Sequential([
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(128, activation='relu'),                          
    tf.keras.layers.Dense(32),    
])

# create the user input and point to the base network
input_user = tf.keras.layers.Input(shape=(num_user_features))
vu = user_NN(input_user)
vu = tf.linalg.l2_normalize(vu, axis=1)

# create the item input and point to the base network
input_item = tf.keras.layers.Input(shape=(num_item_features))
vm = user_NN(input_item)
vm = tf.linalg.l2_normalize(vm, axis=1)

# measure the similarity of the two vector outputs
output = tf.keras.layers.Dot(axes=1)([vu, vm])

# specify the inputs and output of the model 
model = Model([input_user, input_item], output)

# specify the cost function
cost_fn = tf.keras.losses.MeanSquaredError()
```

