---
title: "Reinforcement learning"
categories:
  - MachineLearning 
excerpt: "state action value function"
author_profile: false
sidebar:
    title: Machine Learning
    nav: sidebar_ML
usemathjax: true
---


## Return

- $$R_1 + \gamma*R_2 + \gamma^2*R_3 + \gamma^3*R_4 + \ldots$$ (until terminal state)
- $$R_n$$: reward at state $$n$$
- $$\gamma$$: discount factor (e.g., 0.9)
- the return depends on the action you take

## Policy

- a function $$\pi(s)=a$$ mapping from states to actions, that tells you what action $$a$$ to take in a given state $$s$$
- the goal of reinforcement learning
  - Find a policy $$\pi$$ that tells you what action ($$a=\pi(s)$$) to take in every state ($$s$$) so as to maximize the return

## State action value function

- $$Q(s,a)$$ = return if you
  - start in state $$s$$
  - take action $$a$$ (once)
  - then behave optimally after that
- the best possible return from state $$s$$ is $$\underset{a}{max}\ Q(s,a)$$
- Bellman Equation
  - $$R(s)$$: reward of current state
  - $$s$$: current state
  - $$a$$: current action
  - $$s'$$: state you get to after taking action $$a$$
  - $$a'$$: action that you take in state $$s'$$
  - $$Q(s,a) = R(s) + \gamma \ \underset{a'}{max}\ Q(s',a')$$

## Learning the state-value function

- In a state $$s$$, use neural network to compute
  - $$Q(s,nothing), Q(s,left), Q(s,main), Q(s,right)$$
  - Pick an action $$a$$ that maximize $$Q(s,a)$$
- Input $$\vec{x} = [s; \ a]$$
  - Four possible actions were encoded with one-hot encoder (1,0,0,0), (0,1,0,0), (0,0,1,0), (0,0,0,1)
- Learning algorithim
  - initialize neural network randomly as guess of $$Q(s,a)$$
  - Repeat
    - Take actions in the lunar lander, get $$(s,a,R(s),s')$$
    - Store 10,000 most recent $$(s,a,R(s),s')$$ tuples
    - Train neural network:
      - Create training set of 10,000 examples using
        - $$x=(s,a)$$ and $$y = R(s) + \gamma \ \underset{a'}{max} \ Q(s',a')$$
      - Train $$Q_{new}$$ such that $$Q_{new}(s,a) = y$$
    - Set $$Q = Q_{new}$$
- How to choose actions while still learning
  - because of random initialization, some action could not be learned when we always pick the action $$a$$ that maximize $$Q(s,a)$$
  - In some state $$s$$
    - with probability 0.95, pick the action $$a$$ that maximize $$Q(s,a)$$
    - with probability 0.05, pick the action $$a$$ randomly. "Exploration"
  - $$\epsilon$$: greedy policy (e.g, 0.05)
    - start with high $$\epsilon$$, then gradually decrease (1.0 --\> 0.01)

## Limitations of reinforcement learning

- much easier to get to work in a simulation than a real robot
- Far fewer applications than supervised and unsupervised learning
- But... exciting research direction with potential for future application
