## Terminology
- Training set
	- Data used to train the model
- Notation
	- x: "input" variable feature
	- y: "output (or target)" variable
	- m: number of training examples
	- (x, y): single training example
	- (x<sup>(i)</sup>, y<sup>(i)</sup>): i<sup>th</sup> training example
	- linear model: f<sub>w,b</sub>(x) = wx + b

## Cost function formula
- $\hat{y}^{(i)} = f_{w,b}(x^{(i)})$
- $f_{w,b}(x^{(i)}) = wx^{(i)}+b$
- Find $w, b$:
	- $\hat{y}^{(i)}$ is close to $y^{(i)}$ for all $(x^{(i)},y^{(i)})$
- Squared error cost function	
	- $J(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(\hat{y}^{(i)}-y^{(i)})^2$
	- $J(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})^2$
	- m = number of training examples
	
## Cost function intuition
- Model: $f_{w,b}(x^{(i)}) = wx^{(i)}+b$
- Parameters: $w, b$
- Cost function: $J(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})^2$
- Goal: $\underset{w,b}{minimize} J(w,b)$


